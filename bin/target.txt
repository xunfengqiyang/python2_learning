Preemptive Information Extraction using Unrestricted Relation Discovery

Yusuke Shinyama

Satoshi Sekine

New York University

715, Broadway, 7th Floor

New York, NY, 10003

{yusuke,sekine}@cs.nyu.edu

Abstract

We are trying to extend the boundary ofInformation Extraction (IE) systems. Ex-isting IE systems require a lot of time andhuman effort to tune for a new scenario.Preemptive Information Extraction is anattempt to automatically create all feasibleIE systems in advance without human in-tervention. We propose a technique calledUnrestricted Relation Discovery that dis-covers all possible relations from texts andpresents them as tables. We present a pre-liminary system that obtains reasonablygood results.

1 Background

Every day, a large number of news articles are cre-ated and reported, many of which are unique. Butcertain types of events, such as hurricanes or mur-ders, are reported again and again throughout a year.The goal of Information Extraction, or IE, is to re-trieve a certain type of news event from past articlesand present the events as a table whose columns areﬁlled with a name of a person or company, accord-ing to its role in the event. However, existing IEtechniques require a lot of human labor. First, youhave to specify the type of information you want andcollect articles that include this information. Then,you have to analyze the articles and manually crafta set of patterns to capture these events. Most exist-ing IE research focuses on reducing this burden byhelping people create such patterns. But each timeyou want to extract a different kind of information,you need to repeat the whole process: specify arti-

cles and adjust its patterns, either manually or semi-automatically. There is a bit of a dangerous pitfallhere. First, it is hard to estimate how good the sys-tem can be after months of work. Furthermore, youmight not know if the task is even doable in the ﬁrstplace. Knowing what kind of information is easilyobtained in advance would help reduce this risk.

An IE task can be deﬁned as ﬁnding a relationamong several entities involved in a certain type ofevent. For example, in the MUC-6 managementsuccession scenario, one seeks a relation betweenCOMPANY, PERSON and POST involved with hir-ing/ﬁring events. For each row of an extracted ta-ble, you can always read it as “COMPANY hired(or ﬁred) PERSON for POST.” The relation betweenthese entities is retained throughout the table. Thereare many existing works on obtaining extraction pat-terns for pre-deﬁned relations (Riloff, 1996; Yangar-ber et al., 2000; Agichtein and Gravano, 2000; Sudoet al., 2003).

Unrestricted Relation Discovery is a technique toautomatically discover such relations that repeatedlyappear in a corpus and present them as a table, withabsolutely no human intervention. Unlike most ex-isting IE research, a user does not specify the typeof articles or information wanted. Instead, a systemtries to ﬁnd all the kinds of relations that are reportedmultiple times and can be reported in tabular form.This technique will open up the possibility of try-ing new IE scenarios. Furthermore, the system itselfcan be used as an IE system, since an obtained re-lation is already presented as a table. If this systemworks to a certain extent, tuning an IE system be-comes a search problem: all the tables are alreadybuilt “preemptively.” A user only needs to searchfor a relevant table.

dump

Article2005-09-23 Katrina2005-10-022005-11-20 Gamma

Longwang

be-hitNew OrleansTaiwanFlorida

Keywords: storm, evacuate, coast, rain, hurricane

Table 1: Sample discovered relation.

We implemented a preliminary system for thistechnique and obtained reasonably good perfor-mance. Table 1 is a sample relation that was ex-tracted as a table by our system. The columns of thetable show article dates, names of hurricanes and theplaces they affected respectively. The headers of thetable and its keywords were also extracted automat-ically.

2 Basic IdeaIn Unrestricted Relation Discovery, the discoveryprocess (i.e. creating new tables) can be formulatedas a clustering task. The key idea is to cluster a setof articles that contain entities bearing a similar rela-tion to each other in such a way that we can constructa table where the entities that play the same role areplaced in the same column.

Suppose that there are two articles A and B,and both report hurricane-related news. Article Acontains two entities “Katrina” and “New Orleans”,and article B contains “Longwang” and “Taiwan”.These entities are recognized by a Named Entity(NE) tagger. We want to discover a relation amongthem. First, we introduce a notion called “basicpattern” to form a relation. A basic pattern is apart of the text that is syntactically connected toan entity. Some examples are “X is hit” or “Y’sresidents”. Figure 1 shows several basic patternsconnected to the entities “Katrina” and “New Or-leans” in article A. Similarly, we obtain the basicpatterns for article B. Now, in Figure 2, both enti-ties “Katrina” and “Longwang” have the basic pat-tern “headed” in common. In this case, we connectthese two entities to each other. Furthermore, thereis also a common basic pattern “was-hit” shared by“New Orleans” and “Taiwan”. Now, we found twosets of entities that can be placed in correspondenceat the same time. What does this mean? We can inferthat both entity sets (“Katrina”-“New Orleans” and“Longwang”-“Taiwan”) represent a certain relationthat has something in common: a hurricane name

Figure 1: Obtaining basic patterns.

Figure 2: Finding a similar relation from two articles.

and the place it affected. By ﬁnding multiple par-allel correspondences between two articles, we canestimate the similarity of their relations.

Generally, in a clustering task, one groups itemsby ﬁnding similar pairs. After ﬁnding a pair of arti-cles that have a similar relation, we can bring theminto the same cluster. In this case, we cluster articlesby using their basic patterns as features. However,each basic pattern is still connected to its entity sothat we can extract the name from it. We can con-sider a basic pattern to represent something like the“role” of its entity. In this example, the entities thathad “headed” as a basic pattern are hurricanes, andthe entities that had “was-hit” as a basic pattern arethe places it affected. By using basic patterns, wecan align the entities into the corresponding columnthat represents a certain role in the relation. Fromthis example, we create a two-by-two table, whereeach column represents the roles of the entities, andeach row represents a different article, as shown inthe bottom of Figure 2.

We can extend this table by ﬁnding another article

KatrinaNew Orleansheadedthreatenedis-category-5...was-hithas-been-evacuated-residents...Basic patternsfor entity "Katrina"Basic patternsfor entity "New Orleans"article A1.2.. . .. . .. . .KatrinaNew Orleansheadedthreatenedis-category-5...was-hithas-been-evacuated-residents...article A1.2.. . .. . .. . .LongwangTaiwanhitheadedswirling...’s-coastwas-poundedwas-hit...article B1.2.. . .. . .. . .Common Pattern"stroke"Common Pattern"was-hit"article AKatrinaNew Orleansarticle BLongwangTaiwanObtainedTablein the same manner. In this way, we gradually extenda table while retaining a relation among its columns.In this example, the obtained table is just what an IEsystem (whose task is to ﬁnd a hurricane name andthe affected place) would create.

However, these articles might also include otherthings, which could represent different relations. Forexample, the governments might call for help orsome casualties might have been reported. To ob-tain such relations, we need to choose different en-tities from the articles. Several existing works havetried to extract a certain type of relation by manu-ally choosing different pairs of entities (Brin, 1998;Ravichandran and Hovy, 2002). Hasegawa et al.(2004) tried to extract multiple relations by choos-ing entity types. We assume that we can ﬁnd suchrelations by trying all possible combinations froma set of entities we have chosen in advance; somecombinations might represent a hurricane and gov-ernment relation, and others might represent a placeand its casualties. To ensure that an article can haveseveral different relations, we let each article belongto several different clusters.

In a real-world situation, only using basic patternssometimes gives undesired results. For example,“(President) Bush ﬂew to Texas” and “(Hurricane)Katrina ﬂew to New Orleans” both have a basic pat-tern “flew to” in common, so “Bush” and “Kat-rina” would be put into the same column. But wewant to separate them in different tables. To allevi-ate this problem, we put an additional restriction onclustering. We use a bag-of-words approach to dis-criminate two articles: if the word-based similaritybetween two articles is too small, we do not bringthem together into the same cluster (i.e. table). Weexclude names from the similarity calculation at thisstage because we want to link articles about the sametype of event, not the same instance. In addition, weuse the frequency of each basic pattern to computethe similarity of relations, since basic patterns like“say” or “have” appear in almost every article and itis dangerous to rely on such expressions.

Increasing Basic PatternsIn the above explanation, we have assumed that wecan obtain enough basic patterns from an article.However, the actual number of basic patterns thatone can ﬁnd from a single article is usually not

enough, because the number of sentences is rathersmall in comparison to the variation of expressions.So having two articles that have multiple basic pat-terns in common is very unlikely. We extend thenumber of articles for obtaining basic patterns byusing a cluster of comparable articles that report thesame event instead of a single article. We call thiscluster of articles a “basic cluster.” Using basic clus-ters instead of single articles also helps to increasethe redundancy of data. We can give more conﬁ-dence to repeated basic patterns.

Note that the notion of “basic cluster” is differentfrom the clusters used for creating tables explainedabove. In the following sections, a cluster for creat-ing a table is called a “metacluster,” because this isa cluster of basic clusters. A basic cluster consistsof a set of articles that report the same event whichhappens at a certain time, and a metacluster consistsof a set of events that contain the same relation overa certain period.

We try to increase the number of articles in a basiccluster by looking at multiple news sources simulta-neously. We use a clustering algorithm that uses avector-space-model to obtain basic clusters. Thenwe apply cross-document coreference resolution toconnect entities of different articles within a basiccluster. This way, we can increase the number of ba-sic patterns connected to each entity. Also, it allowsus to give a weight to entities. We calculate theirweights using the number of occurrences within acluster and their position within an article. Theseentities are used to obtain basic patterns later.

We also use a parser and tree normalizer to gen-erate basic patterns. The format of basic patternsis crucial to performance. We think a basic pat-tern should be somewhat speciﬁc, since each pat-tern should capture an entity with some relevant con-text. But at the same time a basic pattern shouldbe general enough to reduce data sparseness. Wechoose a predicate-argument structure as a naturalsolution for this problem. Compared to traditionalconstituent trees, a predicate-argument structure isa higher-level representation of sentences that hasgained wide acceptance from the natural languagecommunity recently. In this paper we used a logicalfeature structure called GLARF proposed by Mey-ers et al. (2001a). A GLARF converter takes a syn-tactic tree as an input and augments it with several

Figure 3: GLARF structure of the sentence “Katrinahit Louisiana’s coast.”

features. Figure 3 shows a sample GLARF structureobtained from the sentence “Katrina hit Louisiana’scoast.” We used GLARF for two reasons: ﬁrst,unlike traditional constituent parsers, GLARF hasan ability to regularize several linguistic phenom-ena such as participial constructions and coordina-tion. This allows us to handle this syntactic varietyin a uniform way. Second, an output structure canbe easily converted into a directed graph that rep-resents the relationship between each word, withoutlosing signiﬁcant information from the original sen-tence. Compared to an ordinary constituent tree, it iseasier to extract syntactic relationships. In the nextsection, we discuss how we used this structure togenerate basic patterns.

3 Implementation

The overall process to generate basic patterns anddiscover relations from unannotated news articles isshown in Figure 4. Theoretically this could be astraight pipeline, but due to the nature of the im-plementation we process some stages separately andcombine them in the later stage.In the followingsubsection, we explain each component.

3.1 Web Crawling and Basic ClusteringFirst of all, we need a lot of news articles from mul-tiple news sources. We created a simple web crawlerthat extract the main texts from web pages. We ob-served that the crawler can correctly take the maintexts from about 90% of the pages from each newssite. We ran the crawler every day on several newssites. Then we applied a simple clustering algorithmto the obtained articles in order to ﬁnd a set of arti-

Figure 4: System overview.

cles that talk about exactly the same news and forma basic cluster.

We eliminate stop words and stem all the otherwords, then compute the similarity between two ar-ticles by using a bag-of-words approach.In newsarticles, a sentence that appears in the beginning ofan article is usually more important than the others.So we preserved the word order to take into accountthe location of each sentence. First we computed aword vector from each article:

(cid:88)

Vw(A) = IDF(w)

exp(−

i

avgwords

)

i∈P OS(w,A)

where Vw(A) is a vector element of word w in articleA, IDF (w) is the inverse document frequency ofword w, and P OS(w, A) is a list of w’s positionsin the article. avgwords is the average number ofwords for all articles. Then we calculated the cosinevalue of each pair of vectors:

Sim(A1, A2) = cos(V (A1) · V (A2))

We computed the similarity of all possible pairs ofarticles from the same day, and selected the pairs

KatrinahitcoastSBJOBJLouisianaT-POS’sSUFFIXWebCrawlingBasicClusteringCoreferenceResolutionParsingGLARFingBasic PatternGenerationMetaclusteringNewspapers......BasicClustersBasic PatternsMetaclusters(Tables)whose similarity exceeded a certain threshold (0.65in this experiment) to form a basic cluster.

3.2 Parsing and GLARFingAfter getting a set of basic clusters, we pass themto an existing statistical parser (Charniak, 2000) andrule-based tree normalizer to obtain a GLARF struc-ture for each sentence in every article. The currentimplementation of a GLARF converter gives about75% F-score using parser output. For the details ofGLARF representation and its conversion, see Mey-ers et al. (2001b).

3.3 NE Tagging and Coreference ResolutionIn parallel with parsing and GLARFing, we also ap-ply NE tagging and coreference resolution for eacharticle in a basic cluster. We used an HMM-basedNE tagger whose performance is about 85% in F-score. This NE tagger produces ACE-type NamedEntities 1: PERSON, ORGANIZATION, GPE, LO-CATION and FACILITY 2. After applying single-document coreference resolution for each article, weconnect the entities among different articles in thesame basic cluster to obtain cross-document coref-erence entities with simple string matching.

3.4 Basic Pattern GenerationAfter getting a GLARF structure for each sentenceand a set of documents whose entities are taggedand connected to each other, we merge the two out-puts and create a big network of GLARF structureswhose nodes are interconnected across different sen-tences/articles. Now we can generate basic patternsfor each entity. First, we compute the weight foreach cross-document entity E in a certain basic clus-ter as follows:

WE =

mentions(e) · exp(−C · f irstsent(e))

(cid:88)

e∈E

where e ∈ E is an entity within one article andmentions(e) and f irstsent(e) are the number ofmentions of entity e in a document and the position

1The ACE task

canhttp://www.itl.nist.gov/iad/894.01/tests/ace/guidelines at http://www.ldc.upenn.edu/Projects/ACE/

description

beand

foundatthe ACE

2The hurricane names used in the examples were recognized

as PERSON.

Figure 5: Basic patterns obtained from the sentence“Katrina hit Louisiana’s coast.”

of the sentence where entity e ﬁrst appeared, respec-tively. C is a constant value which was 0.5 in this ex-periment. To reduce combinatorial complexity, wetook only the ﬁve most highly weighted entities fromeach basic cluster to generate basic patterns. We ob-served these ﬁve entities can cover major relationsthat are reported in a basic cluster.

Next, we obtain basic patterns from the GLARFstructures. We used only the ﬁrst ten sentencesin each article for getting basic patterns, as mostimportant facts are usually written in the ﬁrst fewsentences of a news article. Figure 5 shows allthe basic patterns obtained from the sentence “Ka-trina hit Louisiana’s coast.” The shaded nodes“Katrina” and “Louisiana” are entities from whicheach basic pattern originates. We take a pathof GLARF nodes from each entity node until itreaches any predicative node: noun, verb, or ad-jective in this case.Since the nodes “hit” and“coast” can be predicates in this example, we ob-tain three unique paths “Louisiana+T-POS:coast(Louisiana’s coast)”, “Katrina+SBJ:hit (Katrinahit something)”, and “Katrina+SBJ:hit-OBJ:coast(Katrina hit some coast)”.

To increase the speciﬁcity of patterns, we generateextra basic patterns by adding a node that is imme-diately connected to a predicative node. (From thisexample, we generate two basic patterns: “hit” and“hit-coast” from the “Katrina” node.)

Notice that

in a GLARF structure,

the typeisof each argument such as subject or objectpreserved in an edge even if we extract a sin-gle path of a graph.Now, we replace bothentities “Katrina” and “Louisiana” with variables

KatrinahitcoastSBJOBJLouisianaT-POS’sSUFFIXGPE+T-POS:coastPER+SBJ:hitPER+SBJ:hit-OBJ:coastbased on their NE tags and obtain parameter-ized patterns:“GPE+T-POS:coast (Louisiana’scoast)”, “PER+SBJ:hit (Katrina hit something)”,and “PER+SBJ:hit-OBJ:coast (Katrina hit somecoast)”.

After taking all the basic patterns from every basiccluster, we compute the Inverse Cluster Frequency(ICF) of each unique basic pattern. ICF is similarto the Inverse Document Frequency (IDF) of words,which is used to calculate the weight of each basicpattern for metaclustering.

3.5 MetaclusteringFinally, we can perform metaclustering to obtain ta-bles. We compute the similarity between each basiccluster pair, as seen in Figure 6. XA and XB arethe set of cross-document entities from basic clusterscA and cB, respectively. We examine all possiblemappings of relations (parallel mappings of multi-ple entities) from both basic clusters, and ﬁnd all themappings M whose similarity score exceeds a cer-tain threshold. wordsim(cA, cB) is the bag-of-wordssimilarity of two clusters. As a weighting functionwe used ICF:

weight(p) = − log(

clusters that include p

all clusters

)

We then sort the similarities of all possible pairsof basic clusters, and try to build a metacluster bytaking the most strongly connected pair ﬁrst. Notethat in this process we may assign one basic clus-ter to several different metaclusters. When a link isfound between two basic clusters that were alreadyassigned to a metacluster, we try to put them intoall the existing metaclusters it belongs to. However,we allow a basic cluster to be added only if it canﬁll all the columns in that table. In other words, theﬁrst two basic clusters (i.e. an initial two-row table)determines its columns and therefore deﬁne the re-lation of that table.

4 Experiment and Evaluation

We used twelve newspapers published mainly in theU.S. We collected their articles over two months(from Sep. 21, 2005 - Nov. 27, 2005). We obtained643,767 basic patterns and 7,990 unique types. Thenwe applied metaclustering to these basic clusters

Source articlesBasic clustersBasic patterns (token)Basic patterns (type)MetaclustersMetaclusters (rows ≥ 3)

28,0095,543643,7677,990302101

Table 2: Articles and obtained metaclusters.

and obtained 302 metaclusters (tables). We then re-moved duplicated rows and took only the tables thathad 3 or more rows. Finally we had 101 tables. Thetotal number the of articles and clusters we used areshown in Table 2.

4.1 Evaluation MethodWe evaluated the obtained tables as follows. Foreach row in a table, we added a summary of thesource articles that were used to extract the rela-tion. Then for each table, an evaluator looks intoevery row and its source article, and tries to comeup with a sentence that explains the relation amongits columns. The description should be as speciﬁc aspossible. If at least half of the rows can ﬁt the ex-planation, the table is considered “consistent.” Foreach consistent table, the evaluator wrote down thesentence using variable names ($1, $2, ...) to referto its columns. Finally, we counted the number ofconsistent tables. We also counted how many rowsin each table can ﬁt the explanation.

4.2 ResultsWe evaluated 48 randomly chosen tables. Amongthese tables, we found that 36 tables were consis-tent. We also counted the total number of rows thatﬁt each description, shown in Table 3. Table 4 showsthe descriptions of the selected tables. The largestconsistent table was about hurricanes (Table 5). Al-though we cannot exactly measure the recall of eachtable, we tried to estimate the recall by comparingthis hurricane table to a manually created one (Table6). We found 6 out of 9 hurricanes 3. It is worthnoting that most of these hurricane names were au-tomatically disambiguated although our NE taggerdidn’t distinguish a hurricane name from a person

3Hurricane Katrina and Longwang shown in the previousexamples are not included in this table. They appeared beforethis period.

for each cluster pair (cA, cB) {

XA = cA.entitiesXB = cB.entitiesfor each entity mapping M = [(xA1, xB1), ..., (xAn, xBn)] ∈ (2|XA| × 2|XB|) {

for each entity pair (xAi, xBi) {

Pi = xAi.patterns ∩ xBi.patternsweight(p)pairscorei =

p∈Pi

(cid:80)

(cid:80)

}mapscore =if T1 < |M| and T2 < mapscore and T3 < wordsim(cA.words, cB.words) {}

link cA and cB with mapping M.

pairscorei

}

}

Figure 6: Computing similarity of basic clusters.

Tables:Consistent tablesInconsistent tablesTotalRows:Rows that ﬁt the descriptionRows not ﬁttedTotal

36 (75%)1248

118 (73%)43161

Table 3: Evaluation results.

RowsDescription8/16Storm $1(PER) probably affected $2(GPE).4/7Nominee $2(PER) must be conﬁrmed by $1(ORG).4/6$1(PER) urges $2(GPE) to make changes.3/5$1(GPE) launched an attack in $2(GPE).4/5$1(PER) ran against $2(PER) in an election.2/4$2(PER) visited $1(GPE) on a diplomatic mission.4/4$2(PER) beat $1(PER) in golf.3/3$2(GPE) soldier(s) were killed in $1(GPE).2/3$2(PER) ran for governor of $1(GPE).3/3Boxer $1(PER) fought boxer $2(PER).Table 4: Description of obtained tables and the num-ber of ﬁtted/total rows.

name. The second largest table (about nominationsof ofﬁcials) is shown in Table 7.

We reviewed 10 incorrect rows from various ta-bles and found 4 of them were due to coreference er-rors and one error was due to a parse error. The other4 errors were due to multiple basic patterns distantfrom each other that happened to refer to a differentevent reported in the same cluster. The causes of theone remaining error was obscure. Most inconsistenttables were a mixture of multiple relations and someof their rows still looked consistent.

We have a couple of open questions. First, theoverall recall of our system might be lower than ex-

isting IE systems, as we are relying on a cluster ofcomparable articles rather than a single document todiscover an event. We might be able to improve thisin the future by adjusting the basic clustering algo-rithm or weighting schema of basic patterns. Sec-ondly, some combinations of basic patterns lookedinherently vague. For example, we used the two ba-sic patterns “pitched” and “’s-series” in the fol-lowing sentence (the patterns are underlined):

Ervin Santana pitched 5 1-3 gutsy innings in his post-season debut for the Angels, Adam Kennedy hit a go-ahead triple that sent Yankees outﬁelders crashing to theground, and Los Angeles beat New York 5-3 Mondaynight in the decisive Game 5 of their AL playoff series.

It is not clear whether this set of patterns can yieldany meaningful relation. We are not sure how muchthis sort of table can affect overall IE performance.

5 ConclusionIn this paper we proposed Preemptive InformationExtraction as a new direction of IE research. Asits key technique, we presented Unrestricted Rela-tion Discovery that tries to ﬁnd parallel correspon-dences between multiple entities in a document, andperform clustering using basic patterns as features.To increase the number of basic patterns, we useda cluster of comparable articles instead of a singledocument. We presented the implementation of ourpreliminary system and its outputs. We obtaineddozens of usable tables.

1:dumpArticleRita2005-09-21 (1)Rita2005-09-23Bush2005-09-252005-09-26Damrey2005-09-27 (2) DamreyRita2005-10-01Otis2005-10-02Longwang2005-10-04Stan2005-10-05Tammy2005-10-062005-10-07Tammy2005-10-19 (3) WilmaWilma2005-10-25Wilma2005-10-252005-10-28BetaGamma2005-11-20

2:coastTexasNew OrleansTexasHainanVietnamLouisianaMexicoChinaMexicoFloridaGeorgiaFloridaCubaMassachusettsNicaraguaFlorida

1. More than 2,000 National Guard troops were put onactive-duty alert to assist as Rita slammed into the stringof islands and headed west, perhaps toward Texas. ...

2. Typhoon Damrey smashed into Vietnam on Tuesday af-

ter killing nine people in China, ...

3. Oil markets have been watching Wilma’s progress ner-vously, ... but the threat to energy interests appears tohave eased as forecasters predict the storm will turn to-ward Florida. ...

Table 5: Hurricane table (“Storm $1(PER) probablyaffected $2(GPE).”) and the actual expressions weused for extraction.

Hurricane Date (Affected Place)Philippe* Rita* Stan* TammyVince* WilmaAlpha* Beta* Gamma Nov 13-20 (Belize, etc.)

Sep 17-20 (?)Sep 17-26 (Louisiana, Texas, etc.)Oct 1-5 (Mexico, Nicaragua, etc.)Oct 5-? (Georgia, Alabama)Oct 8-11 (Portugal, Spain)Oct 15-25 (Cuba, Honduras, etc.)Oct 22-24 (Haiti, Dominican Rep.)Oct 26-31 (Nicaragua, Honduras)

Articles6566831812368805536

Table 6: Hurricanes in North America between mid-Sep. and Nov.(from Wikipedia). Rows with astar (*) were actually extracted. The number of thesource articles that contained a mention of the hurri-cane is shown in the right column.

Article2005-09-212005-10-032005-10-202005-10-262005-10-312005-11-042005-11-17

2:be-conﬁrmedRoberts

1:conﬁrmSenateSupreme Court MiersBushSenateSauerbreySenateMr. AlitoSenateAlitoSenateFedBernanke

Table 7: Nomination table (“Nominee $2(PER)must be conﬁrmed by $1(ORG).”)

AcknowledgementsThis research was supported by the National ScienceFoundation under Grant IIS-00325657. This paperdoes not necessarily reﬂect the position of the U.S.Government. We would like to thank Prof. RalphGrishman who provided useful suggestions and dis-cussions.

ReferencesEugene Agichtein and L. Gravano. 2000. Snowball: Ex-tracting Relations from Large Plaintext Collections. InProceedings of the 5th ACM International Conferenceon Digital Libraries (DL-00).

Sergey Brin. 1998. Extracting Patterns and RelationsIn WebDB Workshop at

from the World Wide Web.EDBT ’98.

Eugene Charniak. 2000. A maximum-entropy-inspired

parser. In Proceedings of NAACL-2000.

Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.2004. Discovering relations among named entitiesfrom large corpora.In Proceedings of the AnnualMeeting of Association of Computational Linguistics(ACL-04).

Adam Meyers, Ralph Grishman, Michiko Kosaka, and2001a. Covering Treebanks withIn ACL/EACL Workshop on Sharing Tools

Shubin Zhao.GLARF.and Resources for Research and Education.

Adam Meyers, Michiko Kosaka, Satoshi Sekine, RalphGrishman, and Shubin Zhao. 2001b. Parsing andGLARFing. In Proceedings of RANLP-2001, TzigovChark, Bulgaria.

Deepak Ravichandran and Eduard Hovy. 2002. Learningsurface text patterns for a question answering system.In Proceedings of the 40th Annual Meeting of the As-sociation for Computational Linguistics (ACL).

Ellen Riloff. 1996. Automatically Generating Extrac-tion Patterns from Untagged Text. In Proceedings ofthe 13th National Conference on Artiﬁcial Intelligence(AAAI-96).

Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.2003. An Improved Extraction Pattern Representa-tion Model for Automatic IE Pattern Acquisition. InProceedings of the Annual Meeting of Association ofComputational Linguistics (ACL-03).

Roman Yangarber, Ralph Grishman, Pasi Tapanainen,and Silja Huttunen. 2000. Unsupervised Discoveryof Scenario-Level Patterns for Information Extraction.In Proceedings of the 18th International Conferenceon Computational Linguistics (COLING-00).

